{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Dataset, Reader, SVD, accuracy, AlgoBase \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan Rody\\AppData\\Local\\Temp\\ipykernel_27260\\2654531577.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_books = pd.read_csv('data/Books.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_books = pd.read_csv('data/Books.csv')\n",
    "all_ratings = pd.read_csv('data/Ratings.csv')\n",
    "all_users = pd.read_csv('data/Users.csv')\n",
    "all_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if item is a valid ISBN\n",
    "def is_valid_isbn(isbn):\n",
    "     if len(isbn) != 10: return False\n",
    "     if not isbn.isalnum(): return False\n",
    "\n",
    "     return True\n",
    "\n",
    "# Converts ISBN to numerical ID\n",
    "def convert_isbn_to_id(isbn, isbn_to_id):\n",
    "    return isbn_to_id.get(isbn, None)\n",
    "\n",
    "data_list = []\n",
    "isbn_to_id = {}\n",
    "id_counter = 0\n",
    "\n",
    "# Converts all ISBNs in data to numerical IDs\n",
    "with open('data/Ratings.csv', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        # Skips misformatted items that would cause an error\n",
    "        try:\n",
    "            item_id, isbn, rating = line.strip().split(',')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Skips misformatted items (not valid ISBNs)\n",
    "        if not is_valid_isbn(isbn):\n",
    "            continue\n",
    "\n",
    "        # Converts ISBN to numerical ID\n",
    "        if isbn not in isbn_to_id:\n",
    "            isbn_to_id[isbn] = id_counter\n",
    "            id_counter += 1\n",
    "        \n",
    "        # Builds dictionary\n",
    "        item_id = isbn_to_id[isbn]\n",
    "\n",
    "        # Add data to list of tuples\n",
    "        data_list.append((item_id, item_id, float(rating)))\n",
    "\n",
    "# Creates inverted dictionary, use to convert numerical ID to ISBN for reporting results\n",
    "id_to_isbn = {v: k for k, v in isbn_to_id.items()}\n",
    "\n",
    "# Converts list to pandas data frame\n",
    "df = pd.DataFrame(data_list, columns=['user_id', 'item_id', 'rating'])\n",
    "\n",
    "# Creates Reader to extract data, uses it to load data\n",
    "reader = Reader(line_format=\"user item rating\", sep=',', rating_scale=(1,10))\n",
    "data = Dataset.load_from_df(df, reader=reader)\n",
    "\n",
    "# Splits the data into train_data and test_data\n",
    "train_data, test_data = train_test_split(data, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weighted sum recommendation system algorithm\n",
    "class WeightedSum(AlgoBase):\n",
    "    # Initializes algorithm using a list of models to serve as base\n",
    "    def __init__(self, models, learn_rate=0.01, threshold=1e-5, perturbation=1e-5):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "        self.models = models\n",
    "        self.weights = [1/len(models)]*len(models)\n",
    "        self.learn_rate = learn_rate\n",
    "        self.threshold = threshold\n",
    "        self.perturbation = perturbation\n",
    "    \n",
    "    # Fits algorithm to data, optimizes weights\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        \n",
    "        # Splits trainset into train_data and optimize_data\n",
    "        temp_df = pd.DataFrame(trainset.all_ratings(), columns=['user_id', 'item_id', 'rating'])\n",
    "        temp_data = Dataset.load_from_df(temp_df, reader=reader)\n",
    "        train_data, optimize_data = train_test_split(temp_data, test_size=0.4)\n",
    "\n",
    "        # Fits base models to train_data\n",
    "        for model in self.models:\n",
    "            model.fit(train_data)\n",
    "        \n",
    "        # Optimizes weights using optimize_data through gradient descent\n",
    "        change = 1\n",
    "\n",
    "        pred = self.test(optimize_data)\n",
    "        acc = accuracy.rmse(pred, False)\n",
    "        prev_acc = acc\n",
    "        while change > self.threshold:\n",
    "            # Copies current state of weights\n",
    "            curr_weights = self.weights.copy()\n",
    "            \n",
    "            # Computes gradients\n",
    "            gradients = []\n",
    "            for i in range(len(self.weights)):\n",
    "                weight = self.weights[i]\n",
    "\n",
    "                self.weights[i] += self.perturbation\n",
    "                new_pred = self.test(optimize_data)\n",
    "                new_acc = accuracy.rmse(new_pred, False)\n",
    "\n",
    "                self.weights[i] = weight\n",
    "                pred = self.test(optimize_data)\n",
    "                acc = accuracy.rmse(pred, False)\n",
    "\n",
    "                gradient = (new_acc - acc) / self.perturbation\n",
    "                gradients.append(gradient)\n",
    "\n",
    "            # Restores state of weights\n",
    "            self.weights = curr_weights\n",
    "\n",
    "            # Modifies weights\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.learn_rate * gradients[i]\n",
    "                # Minimum value for weight is 0\n",
    "                self.weights[i] = max(self.weights[i], 0)\n",
    "\n",
    "            # Ensures weights sum to 1\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i] / sum(self.weights)\n",
    "\n",
    "            # Computes new accuracy\n",
    "            pred = self.test(optimize_data)\n",
    "            acc = accuracy.rmse(pred, False)\n",
    "            change = abs(acc - prev_acc)\n",
    "            prev_acc = acc\n",
    "        \n",
    "        return self\n",
    "\n",
    "    # Estimates rating for given user and item\n",
    "    def estimate(self, u , i):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            predictions.append(model.predict(u, i))\n",
    "\n",
    "        final_prediction = 0\n",
    "        for i in range(0, len(self.weights)):\n",
    "            final_prediction += predictions[i].est*self.weights[i]\n",
    "\n",
    "        return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Recommendation system algorithm that estimates rating as global mean\n",
    "class GlobalMean(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        self.mean_rating = trainset.global_mean\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        return self.mean_rating\n",
    "\n",
    "# Custom Recommendation system algorithm that estimates rating as mean rating of the user\n",
    "class UserMean(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        self.means = {}\n",
    "        for user_id in trainset.all_users():\n",
    "            user_ratings = [0]\n",
    "            for rating in trainset.ur[user_id]:\n",
    "                _, score = rating\n",
    "                if score != 0:\n",
    "                    user_ratings.append(score)\n",
    "\n",
    "            # If user has no ratings: estimate is global mean\n",
    "            if len(user_ratings) == 1:\n",
    "                user_mean = trainset.global_mean\n",
    "            else:\n",
    "                user_mean = sum(user_ratings) / (len(user_ratings)-1)         \n",
    "\n",
    "            self.means[user_id] = user_mean\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return self.means.get(u, self.trainset.global_mean)\n",
    "\n",
    "# Custom Recommendation system algorithm that estimates rating as mean rating of the item\n",
    "class ItemMean(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        self.means = {}\n",
    "        for item_id in trainset.all_items():\n",
    "            item_ratings = [0]\n",
    "            for rating in trainset.ir[item_id]:\n",
    "                _, score = rating\n",
    "                if score != 0:\n",
    "                    item_ratings.append(score)\n",
    "\n",
    "            # If item has no ratings: estimate is global mean\n",
    "            if len(item_ratings) == 1:\n",
    "                item_mean = trainset.global_mean\n",
    "            else:\n",
    "                item_mean = sum(item_ratings) / (len(item_ratings)-1)         \n",
    "\n",
    "            self.means[item_id] = item_mean\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return self.means.get(i, self.trainset.global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean Model Accuracy: 3.85268\n",
      "User Mean Model Accuracy: 5.04057\n",
      "Item Mean Model Accuracy: 5.04057\n",
      "SVD Model Accuracy: 3.91872\n"
     ]
    }
   ],
   "source": [
    "gm = GlobalMean()\n",
    "um = UserMean()\n",
    "im = ItemMean()\n",
    "svd = SVD()\n",
    "\n",
    "gm.fit(train_data)\n",
    "um.fit(train_data)\n",
    "im.fit(train_data)\n",
    "svd.fit(train_data)\n",
    "\n",
    "gm_pred = gm.test(test_data)\n",
    "um_pred = um.test(test_data)\n",
    "im_pred = im.test(test_data)\n",
    "svd_pred = svd.test(test_data)\n",
    "\n",
    "print(\"Global Mean Model Accuracy: {:.5f}\".format(accuracy.rmse(gm_pred, False)))\n",
    "print(\"User Mean Model Accuracy: {:.5f}\".format(accuracy.rmse(um_pred, False)))\n",
    "print(\"Item Mean Model Accuracy: {:.5f}\".format(accuracy.rmse(im_pred, False)))\n",
    "print(\"SVD Model Accuracy: {:.5f}\".format(accuracy.rmse(svd_pred, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.WeightedSum at 0x173966e6710>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declares models for use in weighted sum (NOTE: this takes ~145min)\n",
    "svd = SVD()\n",
    "gm = GlobalMean()\n",
    "um = UserMean()\n",
    "im = ItemMean()\n",
    "\n",
    "models = [svd, gm, um, im]\n",
    "\n",
    "# Declares weighted sum, fits to train_data\n",
    "weighted_sum = WeightedSum(models)\n",
    "weighted_sum.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.8585\n",
      "Optimal Weights:  [0.5725664389874676, 0.4268179940139243, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Reports accuracy\n",
    "predictions = weighted_sum.test(test_data)\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "print(\"Optimal Weights: \", weighted_sum.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free\n",
      "Die Gefahrten I\n",
      "Wild\n",
      "2001: A Space Odyssey\n",
      "Dogs and Their Women\n"
     ]
    }
   ],
   "source": [
    "#converts prediction vector to np.array\n",
    "arr = np.array(predictions)\n",
    "pred_arr = np.array([i[:4] for i in arr])\n",
    "#gets the values user id, item id, user's rating of item, and prediction score (can be expanded for more data)\n",
    "df = pd.DataFrame(pred_arr, columns=['uid','iid','rating','score'])\n",
    "#drop duplicate ISBN to prevent repeat predictions (currently most likely drops duplicate ISBNs without a rating first)\n",
    "df = df.drop_duplicates('iid')\n",
    "df[\"ISBN\"] = df['iid'].apply(lambda x: id_to_isbn[x])\n",
    "df = df.sort_values(\"score\", ascending = False)\n",
    "for isbn in df['ISBN'][:5]:\n",
    "    recc_book = all_books.loc[all_books[\"ISBN\"] == isbn]['Book-Title'].to_string(index = False)\n",
    "    print(recc_book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
